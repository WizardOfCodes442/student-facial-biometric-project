{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrEtYlaXdSeiq7W2sUxbq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WizardOfCodes442/student-facial-biometric-project/blob/main/modeltestmd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwXyQ4X6cVGR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ki7PKFA6chCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"intro\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# Facial Recognition CNN Model Testing\\n\",\n",
        "    \"\\n\",\n",
        "    \"This notebook demonstrates the implementation and testing of our CNN model for facial recognition in a student examination environment.\\n\",\n",
        "    \"\\n\",\n",
        "    \"## Setup and Dependencies\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"setup\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"!pip install tensorflow opencv-python-headless scikit-learn pandas matplotlib\\n\",\n",
        "    \"\\n\",\n",
        "    \"import tensorflow as tf\\n\",\n",
        "    \"from tensorflow.keras.applications import MobileNetV2\\n\",\n",
        "    \"from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\\n\",\n",
        "    \"from tensorflow.keras import layers, models\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import cv2\\n\",\n",
        "    \"import matplotlib.pyplot as plt\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"from google.colab import files\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Check if GPU is available\\n\",\n",
        "    \"print(\\\"Num GPUs Available: \\\", len(tf.config.list_physical_devices('GPU')))\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"model-architecture\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Model Architecture\\n\",\n",
        "    \"\\n\",\n",
        "    \"We'll implement the CNN model as described in the original architecture.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"cnn-model\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"INPUT_SHAPE = (224, 224, 3)\\n\",\n",
        "    \"\\n\",\n",
        "    \"def create_model(num_classes):\\n\",\n",
        "    \"    base_model = MobileNetV2(\\n\",\n",
        "    \"        input_shape=INPUT_SHAPE,\\n\",\n",
        "    \"        include_top=False,\\n\",\n",
        "    \"        weights='imagenet'\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"    \\n    for layer in base_model.layers:\\n\",\n",
        "    \"        layer.trainable = False\\n\",\n",
        "    \"    \n",
        "    \"    model = models.Sequential([\\n\",\n",
        "    \"        base_model,\\n\",\n",
        "    \"        layers.GlobalAveragePooling2D(),\\n\",\n",
        "    \"        layers.Dense(256, activation='relu'),\\n\",\n",
        "    \"        layers.BatchNormalization(),\\n\",\n",
        "    \"        layers.Dropout(0.5),\\n\",\n",
        "    \"        layers.Dense(num_classes, activation='softmax')\\n\",\n",
        "    \"    ])\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    return model\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Compile model\\n\",\n",
        "    \"def compile_model(model):\\n\",\n",
        "    \"    model.compile(\\n\",\n",
        "    \"        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\\n\",\n",
        "    \"        loss='categorical_crossentropy',\\n\",\n",
        "    \"        metrics=['accuracy']\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"    return model\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"data-preparation\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Data Preparation\\n\",\n",
        "    \"\\n\",\n",
        "    \"We'll create functions to load and preprocess our dataset.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"data-functions\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"def load_and_preprocess_image(image_path):\\n\",\n",
        "    \"    img = cv2.imread(image_path)\\n\",\n",
        "    \"    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Face detection\\n\",\n",
        "    \"    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\\n\",\n",
        "    \"    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\\n\",\n",
        "    \"    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    if len(faces) > 0:\\n\",\n",
        "    \"        x, y, w, h = faces[0]\\n\",\n",
        "    \"        face = img[y:y+h, x:x+w]\\n\",\n",
        "    \"    else:\\n\",\n",
        "    \"        face = img\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    face = cv2.resize(face, (INPUT_SHAPE[0], INPUT_SHAPE[1]))\\n\",\n",
        "    \"    face = preprocess_input(face)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    return face\\n\",\n",
        "    \"\\n\",\n",
        "    \"def prepare_dataset(data_dir):\\n\",\n",
        "    \"    images = []\\n\",\n",
        "    \"    labels = []\\n\",\n",
        "    \"    label_mapping = {}\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    for idx, student_folder in enumerate(os.listdir(data_dir)):\\n\",\n",
        "    \"        label_mapping[idx] = student_folder\\n\",\n",
        "    \"        student_path = os.path.join(data_dir, student_folder)\\n\",\n",
        "    \"        \\n\",\n",
        "    \"        if os.path.isdir(student_path):\\n\",\n",
        "    \"            for image_file in os.listdir(student_path):\\n\",\n",
        "    \"                image_path = os.path.join(student_path, image_file)\\n\",\n",
        "    \"                try:\\n\",\n",
        "    \"                    processed_image = load_and_preprocess_image(image_path)\\n\",\n",
        "    \"                    images.append(processed_image)\\n\",\n",
        "    \"                    labels.append(idx)\\n\",\n",
        "    \"                except Exception as e:\\n\",\n",
        "    \"                    print(f\\\"Error processing {image_path}: {e}\\\")\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    return np.array(images), np.array(labels), label_mapping\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"data-upload\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Data Upload and Processing\\n\",\n",
        "    \"\\n\",\n",
        "    \"Upload your dataset to Colab. The dataset should be organized with one folder per student, containing their facial images.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"upload-data\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# Mount Google Drive if your dataset is there\\n\",\n",
        "    \"from google.colab import drive\\n\",\n",
        "    \"drive.mount('/content/drive')\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Update this path to where your dataset is located\\n\",\n",
        "    \"DATA_DIR = '/content/drive/MyDrive/student_dataset'\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Prepare dataset\\n\",\n",
        "    \"X, y, label_mapping = prepare_dataset(DATA_DIR)\\n\",\n",
        "    \"num_classes = len(label_mapping)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Convert labels to one-hot encoding\\n\",\n",
        "    \"y_onehot = tf.keras.utils.to_categorical(y, num_classes)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Split dataset\\n\",\n",
        "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
        "    \"    X, y_onehot, test_size=0.2, random_state=42\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"Training samples: {len(X_train)}\\\")\\n\",\n",
        "    \"print(f\\\"Test samples: {len(X_test)}\\\")\\n\",\n",
        "    \"print(f\\\"Number of classes: {num_classes}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"training\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Model Training\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"train-model\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# Create and compile model\\n\",\n",
        "    \"model = create_model(num_classes)\\n\",\n",
        "    \"model = compile_model(model)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Train model\\n\",\n",
        "    \"history = model.fit(\\n\",\n",
        "    \"    X_train, y_train,\\n\",\n",
        "    \"    validation_split=0.2,\\n\",\n",
        "    \"    epochs=20,\\n\",\n",
        "    \"    batch_size=32\\n\",\n",
        "    \")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"evaluation\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Model Evaluation\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"evaluate-model\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# Plot training history\\n\",\n",
        "    \"plt.figure(figsize=(12, 4))\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.subplot(1, 2, 1)\\n\",\n",
        "    \"plt.plot(history.history['accuracy'], label='Training Accuracy')\\n\",\n",
        "    \"plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\\n\",\n",
        "    \"plt.title('Model Accuracy')\\n\",\n",
        "    \"plt.xlabel('Epoch')\\n\",\n",
        "    \"plt.ylabel('Accuracy')\\n\",\n",
        "    \"plt.legend()\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.subplot(1, 2, 2)\\n\",\n",
        "    \"plt.plot(history.history['loss'], label='Training Loss')\\n\",\n",
        "    \"plt.plot(history.history['val_loss'], label='Validation Loss')\\n\",\n",
        "    \"plt.title('Model Loss')\\n\",\n",
        "    \"plt.xlabel('Epoch')\\n\",\n",
        "    \"plt.ylabel('Loss')\\n\",\n",
        "    \"plt.legend()\\n\",\n",
        "    \"\\n\",\n",
        "    \"plt.tight_layout()\\n\",\n",
        "    \"plt.show()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Evaluate on test set\\n\",\n",
        "    \"test_loss, test_accuracy = model.evaluate(X_test, y_test)\\n\",\n",
        "    \"print(f\\\"Test accuracy: {test_accuracy:.4f}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"testing\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Test Individual Images\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"test-individual\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"def test_image(image_path):\\n\",\n",
        "    \"    # Load and preprocess image\\n\",\n",
        "    \"    processed_image = load_and_preprocess_image(image_path)\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Make prediction\\n\",\n",
        "    \"    prediction = model.predict(np.expand_dims(processed_image, axis=0))\\n\",\n",
        "    \"    predicted_class = np.argmax(prediction[0])\\n\",\n",
        "    \"    confidence = prediction[0][predicted_class]\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Display results\\n\",\n",
        "    \"    plt.figure(figsize=(8, 4))\\n\",\n",
        "    \"    plt.subplot(1, 2, 1)\\n\",\n",
        "    \"    plt.imshow(cv2.imread(image_path)[...,::-1])\\n\",\n",
        "    \"    plt.title('Input Image')\\n\",\n",
        "    \"    plt.axis('off')\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    plt.subplot(1, 2, 2)\\n\",\n",
        "    \"    plt.imshow(processed_image / 2 + 0.5)  # Denormalize\\n\",\n",
        "    \"    plt.title(f'Processed Image\\\\nPredicted: {label_mapping[predicted_class]}\\\\nConfidence: {confidence:.4f}')\\n\",\n",
        "    \"    plt.axis('off')\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    plt.show()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test an individual image\\n\",\n",
        "    \"# You can upload an image using files.upload() or specify a path\\n\",\n",
        "    \"uploaded = files.upload()\\n\",\n",
        "    \"for filename in uploaded.keys():\\n\",\n",
        "    \"    test_image(filename)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {\n",
        "    \"id\": \"save-model\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"## Save Model\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": null,\n",
        "   \"metadata\": {\n",
        "    \"id\": \"save\"\n",
        "   },\n",
        "   \"source\": [\n",
        "    \"# Save model\\n\",\n",
        "    \"model.save('facial_recognition_model.h5')\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Save label mapping\\n\",\n",
        "    \"import json\\n\",\n",
        "    \"with open('label_mapping.json', 'w') as f:\\n\",\n",
        "    \"    json.dump({str(k): v for k, v in label_mapping.items()}, f)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Download files\\n\",\n",
        "    \"files.download('facial_recognition_model.h5')\\n\",\n",
        "    \"files.download('label_mapping.json')\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"accelerator\": \"GPU\",\n",
        "  \"colab\": {\n",
        "   \"private_outputs\": true,\n",
        "   \"provenance\": []\n",
        "  },\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.8.0\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 4\n",
        "}"
      ],
      "metadata": {
        "id": "vzweBmshc6Rb"
      }
    }
  ]
}