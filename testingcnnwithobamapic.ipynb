{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUNBkdda/ErmrpKh3k9MvZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WizardOfCodes442/student-facial-biometric-project/blob/main/testingcnnwithobamapic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfP2DBrh5peX",
        "outputId": "668ba934-14c0-4d62-d076-31d0003999fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Match: Same person\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the MobileNetV2 model\n",
        "def load_model():\n",
        "    base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(128, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Download an image from a URL\n",
        "def download_image(url):\n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        image = np.asarray(bytearray(response.raw.read()), dtype=\"uint8\")\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "        return image\n",
        "    else:\n",
        "        print(f\"Error downloading image from URL: {url}\")\n",
        "        return None\n",
        "\n",
        "# Preprocess image for the model\n",
        "def preprocess_image(image):\n",
        "    resized_image = cv2.resize(image, (224, 224))\n",
        "    processed_image = preprocess_input(resized_image)\n",
        "    return processed_image\n",
        "\n",
        "# Extract face encoding from the image\n",
        "def extract_face_encoding(model, image):\n",
        "    processed_image = preprocess_image(image)\n",
        "    processed_image = np.expand_dims(processed_image, axis=0)  # Add batch dimension\n",
        "    encoding = model.predict(processed_image)\n",
        "    return encoding.flatten()\n",
        "\n",
        "# Load the face detection model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Detect and extract face from an image\n",
        "def detect_face(image):\n",
        "    if image is None:\n",
        "        return None\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray_image, 1.1, 4)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        return None\n",
        "\n",
        "    x, y, w, h = faces[0]  # Take the first detected face\n",
        "    return image[y:y+h, x:x+w]  # Return cropped face\n",
        "\n",
        "# URLs for 3 training images and 1 verification image (replace with your actual URLs)\n",
        "train_image_urls = [\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRxJLWdWHgf6_XGwYB1tTfJvxZ9AHDA2sElJaF_7cvPm4BzsTlKDGOqYyc9S7FJBX-p-fU&usqp=CAU\",\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTCcgE2H9WH7xph_Y2xrr85ltidewM9gcWknw&s\",\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRb4J98Mji_p2b65SRtlxg6tOYXhp3dBX2v8A&s\",\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS9lXHJHGcWiBbJ_K5DON4raTwnowEoz2RDvSze8waMy6xwmBJ0DBTAlsViz7e5ooZshzM&usqp=CAU\",\n",
        "    \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTn6l_iqbERyP8ccI2w0XbcdH00b1ik6I8GMbb0vS0E4_xCw_a_HqEBWgIeQ2oOzXJibHw&usqp=CAU\",\n",
        "    \"https://politico.eu/cdn-cgi/image/width=1160,height=703,fit=crop,quality=80,onerror=redirect,format=auto/wp-content/uploads/2020/05/GettyImages-1225292516.jpg\",\n",
        "    \"https://media.licdn.com/dms/image/C4E12AQFLaKy8cAAxvQ/article-cover_image-/0/1520103496443?e=2147483647&v=beta&t=AXoewqNsSTqdFHnUjGYId2WXnbX8wfaw9n_wcihrAkI\"\n",
        "]\n",
        "verify_image_url = \"https://i.ytimg.com/vi/BdjoHA5ocwU/hq720.jpg?sqp=-oaymwEhCK4FEIIDSFryq4qpAxMIARUAAAAAGAElAADIQj0AgKJD&rs=AOn4CLBzFCX11mQ0tmO692NyJopkOnlT5Q\"\n",
        "# Load the MobileNetV2 model\n",
        "model = load_model()\n",
        "\n",
        "# Load and process the training images\n",
        "train_encodings = []\n",
        "for url in tshrink_600_2000rain_image_urls:\n",
        "    train_image = download_image(url)\n",
        "\n",
        "    if train_image is None:\n",
        "        print(f\"Failed to download or process training image from {url}\")\n",
        "        continue\n",
        "\n",
        "    train_face = detect_face(train_image)\n",
        "\n",
        "    if train_face is not None:\n",
        "        train_encoding = extract_face_encoding(model, train_face)\n",
        "        train_encodings.append(train_encoding)\n",
        "    else:\n",
        "        print(f\"No face detected in training image from {url}\")\n",
        "\n",
        "# Average the encodings from the 3 training images\n",
        "if len(train_encodings) ==7:\n",
        "    averaged_encoding = np.mean(train_encodings, axis=0)\n",
        "else:\n",
        "    print(\"Not enough valid training images with detected faces.\")\n",
        "    exit()\n",
        "\n",
        "# Load and process the verification image\n",
        "verify_image = download_image(verify_image_url)\n",
        "verify_face = detect_face(verify_image)\n",
        "\n",
        "if verify_face is None:\n",
        "    print(\"No face detected in the verification image.\")\n",
        "else:\n",
        "    # Extract encoding for the verification image\n",
        "    verify_encoding = extract_face_encoding(model, verify_face)\n",
        "\n",
        "    # Compare the averaged encoding with the verification encoding\n",
        "    similarity = cosine_similarity([averaged_encoding], [verify_encoding])[0][0]\n",
        "\n",
        "    # Define a threshold (0.9 is often a good threshold for face matching)\n",
        "    if similarity > 0.9:\n",
        "        print(\"Match: Same person\")\n",
        "    else:\n",
        "        print(\"Different people\")\n",
        "\n"
      ]
    }
  ]
}